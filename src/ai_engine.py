import ast
import os

import joblib
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score

from src import config


class TutorAI:
    def __init__(self):
        self.xgb_model: xgb.XGBClassifier | None = None
        self.label_encoder: LabelEncoder | None = None
        self.le_file_path = os.path.join(config.MODEL_DIR, "label_encoder.pkl")

    def batch_evaluate_risk(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‚ºå…¨ç­å­¸ç”Ÿç”Ÿæˆé¢¨éšªç‡ˆè™Ÿèˆ‡ç•¶å‰ç‹€æ…‹"""
        df_out = df.copy()
        
        # ç¢ºä¿æœ‰é æ¸¬çµæœ
        if config.COL_RECOMMENDED_LEVEL not in df_out.columns:
            # å¦‚æœé‚„æ²’è·‘éæ¨¡å‹ï¼Œå…ˆè·‘ä¸€æ¬¡ç°¡å–®è¦å‰‡
            df_out[config.COL_RECOMMENDED_LEVEL] = df_out.apply(self._get_expert_label, axis=1)

        def assess_risk(row):
            acc = row.get(config.COL_ACCURACY, 0)
            att = row.get(config.COL_ATTENDANCE, 0)
            
            # é¢¨éšªé‚è¼¯ï¼šæº–ç¢ºç‡ä½ æˆ– å‡ºå¸­ç‡ä½ -> ç´…ç‡ˆ
            if acc < 0.6 or att < 0.7:
                return "ğŸ”´ High Risk"
            elif acc < 0.75:
                return "ğŸŸ¡ Warning"
            else:
                return "ğŸŸ¢ On Track"

        df_out[config.COL_RISK_LEVEL] = df_out.apply(assess_risk, axis=1)
        
        # æ¨¡æ“¬åˆ†é…ã€Œç•¶å‰ç« ç¯€ã€(éš¨æ©Ÿåˆ†é…çµ¦ demo ç”¨)
        # åœ¨çœŸå¯¦ç³»çµ±ä¸­ï¼Œé€™æœƒå¾è³‡æ–™åº«è®€å–
        import random
        topics = ["M101-åŸºç¤ä»£æ•¸", "M102-å¹¾ä½•åœ–å½¢", "E201-é–±è®€ç†è§£", "M103-é€²éšæ‡‰ç”¨"]
        df_out[config.COL_CURRENT_TOPIC] = [random.choice(topics) for _ in range(len(df_out))]
        
        return df_out

    def _get_expert_label(self, row: pd.Series) -> str:
        """ä¾æ“šç°¡å–®è¦å‰‡ç”Ÿæˆç›£ç£æ¨™ç±¤ã€‚"""
        acc = row.get(config.COL_ACCURACY, 0)
        att = row.get(config.COL_ATTENDANCE, 0)
        hw = row.get(config.COL_HW_COMPLETION, 0)
        score_sum = acc + att + hw

        if score_sum > 2.5:
            return "Hard (Challenge)"
        if score_sum > 1.8:
            return "Medium (Standard)"
        return "Easy (Review)"

    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """è§£æéŒ¯èª¤å‹åˆ¥èˆ‡åˆ†æ•¸åˆ—è¡¨ä¸¦æ¸…æ´—æ•¸å€¼æ¬„ä½ã€‚"""
        df_proc = df.copy()

        def parse_errors(val, key):
            try:
                if isinstance(val, str):
                    parsed = ast.literal_eval(val)
                else:
                    parsed = val
                return float(parsed.get(key, 0))
            except Exception:
                return 0.0

        if config.COL_ERROR_TYPES in df_proc.columns:
            df_proc["err_reading"] = df_proc[config.COL_ERROR_TYPES].apply(
                lambda x: parse_errors(x, "reading")
            )
            df_proc["err_vocab"] = df_proc[config.COL_ERROR_TYPES].apply(
                lambda x: parse_errors(x, "vocab")
            )
            df_proc["err_logic"] = df_proc[config.COL_ERROR_TYPES].apply(
                lambda x: parse_errors(x, "logic")
            )
        else:
            df_proc["err_reading"] = 0.0
            df_proc["err_vocab"] = 0.0
            df_proc["err_logic"] = 0.0

        def calc_mean_score(val):
            try:
                if isinstance(val, str):
                    scores = ast.literal_eval(val)
                else:
                    scores = val
                if isinstance(scores, list) and scores:
                    return float(np.mean(scores))
                return 0.0
            except Exception:
                return 0.0

        if config.COL_SCORE_HISTORY in df_proc.columns:
            df_proc["mean_score"] = df_proc[config.COL_SCORE_HISTORY].apply(calc_mean_score)
        elif config.COL_AVG_SCORE in df_proc.columns:
            df_proc["mean_score"] = pd.to_numeric(
                df_proc[config.COL_AVG_SCORE], errors="coerce"
            ).fillna(0.0)
        else:
            df_proc["mean_score"] = 0.0

        num_cols = [
            config.COL_ACCURACY,
            config.COL_AVG_TIME,
            config.COL_LEARNING_PACE,
            config.COL_ATTENDANCE,
            config.COL_HW_COMPLETION,
        ]

        for col in num_cols:
            if col in df_proc.columns:
                df_proc[col] = pd.to_numeric(df_proc[col], errors="coerce")

        critical_cols = num_cols + ["mean_score", "err_reading", "err_vocab", "err_logic"]
        valid_check_cols = [col for col in critical_cols if col in df_proc.columns]
        df_proc = df_proc.dropna(subset=valid_check_cols)

        return df_proc

    def _get_feature_columns(self) -> list[str]:
        return [
            config.COL_ACCURACY,
            config.COL_AVG_TIME,
            config.COL_LEARNING_PACE,
            config.COL_ATTENDANCE,
            config.COL_HW_COMPLETION,
            "mean_score",
            "err_reading",
            "err_vocab",
            "err_logic",
        ]

    def train_prediction_model(
        self,
        df: pd.DataFrame,
        test_size: float = 0.2,
        n_estimators: int = 100,
        max_depth: int = 5,
        learning_rate: float = 0.1,
        subsample: float = 1.0,
        colsample_bytree: float = 1.0,
        use_cv: bool = False,
        cv_folds: int = 5,
        return_details: bool = False
    ) -> pd.DataFrame | tuple[pd.DataFrame, dict]:
        """
        è¨“ç·´ XGBoost åˆ†é¡æ¨¡å‹ä»¥é æ¸¬æ¨è–¦é›£åº¦ã€‚

        åƒæ•¸:
            df: è¼¸å…¥è³‡æ–™
            test_size: é©—è­‰é›†æ¯”ä¾‹ (0.0-0.5)
            n_estimators: Boosting è¿­ä»£æ¬¡æ•¸
            max_depth: æ¨¹çš„æœ€å¤§æ·±åº¦
            learning_rate: å­¸ç¿’ç‡
            subsample: æ¯æ¬¡è¿­ä»£çš„æ¨£æœ¬æ¡æ¨£æ¯”ä¾‹
            colsample_bytree: æ¯æ£µæ¨¹çš„ç‰¹å¾µæ¡æ¨£æ¯”ä¾‹
            use_cv: æ˜¯å¦åŸ·è¡Œäº¤å‰é©—è­‰
            cv_folds: äº¤å‰é©—è­‰çš„æŠ˜æ•¸
            return_details: æ˜¯å¦è¿”å›è¨“ç·´è©³æƒ…ï¼ˆç”¨æ–¼é€²éšUIï¼‰

        è¿”å›:
            å¦‚æœ return_details=False: è¿”å› df_aligned (å‘å¾Œå…¼å®¹)
            å¦‚æœ return_details=True: è¿”å› (df_aligned, training_info)
        """
        df_proc = self._preprocess_data(df)
        if df_proc.empty:
            if return_details:
                return df, {}
            return df

        features = df_proc[self._get_feature_columns()]
        df_aligned = df.loc[df_proc.index].copy()

        if config.COL_RECOMMENDED_LEVEL not in df_aligned.columns:
            labels = df_aligned.apply(self._get_expert_label, axis=1)
            df_aligned[config.COL_RECOMMENDED_LEVEL] = labels
        else:
            labels = df_aligned[config.COL_RECOMMENDED_LEVEL]

        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(labels)

        # åˆå§‹åŒ–è¨“ç·´ä¿¡æ¯å­—å…¸
        training_info = {}

        # Train/Val Split (ç”¨æ–¼å¯è¦–åŒ–è¨“ç·´éç¨‹)
        X_train, X_val, y_train, y_val = train_test_split(
            features, y_encoded,
            test_size=test_size,
            random_state=config.RANDOM_SEED,
            stratify=y_encoded
        )

        # è¨­å®š XGBoost æ¨¡å‹
        self.xgb_model = xgb.XGBClassifier(
            n_estimators=int(n_estimators),
            learning_rate=float(learning_rate),
            max_depth=int(max_depth),
            subsample=float(subsample),
            colsample_bytree=float(colsample_bytree),
            random_state=config.RANDOM_SEED,
            eval_metric=["mlogloss", "merror"],
            early_stopping_rounds=10
        )

        # è¨“ç·´æ¨¡å‹ï¼ˆè¨˜éŒ„ train & val çš„ metricsï¼‰
        self.xgb_model.fit(
            X_train, y_train,
            eval_set=[(X_train, y_train), (X_val, y_val)],
            verbose=False
        )

        # å„²å­˜æ¨¡å‹
        joblib.dump(self.xgb_model, config.MODEL_FILE_PREDICTION)
        joblib.dump(self.label_encoder, self.le_file_path)

        # å¦‚æœéœ€è¦è¿”å›è©³ç´°ä¿¡æ¯
        if return_details:
            # 1. è¨“ç·´æ­·ç¨‹
            training_info['history'] = self.xgb_model.evals_result()

            # 2. é©—è­‰é›†æº–ç¢ºç‡
            y_pred = self.xgb_model.predict(X_val)
            training_info['val_accuracy'] = accuracy_score(y_val, y_pred)

            # 3. ç‰¹å¾µé‡è¦æ€§
            feature_names = self._get_feature_columns()
            importance = self.xgb_model.feature_importances_
            training_info['feature_importance'] = dict(zip(feature_names, importance))

            # 4. Cross-Validation (å¯é¸)
            if use_cv:
                # å‰µå»ºä¸€å€‹ä¸å¸¶ early_stopping çš„è‡¨æ™‚æ¨¡å‹ç”¨æ–¼ CV
                # å› ç‚º cross_val_score å…§éƒ¨ fit() æ™‚ä¸æœƒæä¾› eval_set
                cv_model = xgb.XGBClassifier(
                    n_estimators=int(n_estimators),
                    learning_rate=float(learning_rate),
                    max_depth=int(max_depth),
                    subsample=float(subsample),
                    colsample_bytree=float(colsample_bytree),
                    random_state=config.RANDOM_SEED,
                    eval_metric="mlogloss"
                    # æ³¨æ„ï¼šé€™è£¡ä¸è¨­ç½® early_stopping_rounds
                )

                cv_scores = cross_val_score(
                    cv_model, features, y_encoded,
                    cv=cv_folds,
                    scoring='accuracy'
                )
                training_info['cv_scores'] = cv_scores.tolist()
                training_info['cv_mean'] = float(np.mean(cv_scores))
                training_info['cv_std'] = float(np.std(cv_scores))

            return df_aligned, training_info

        # å‘å¾Œå…¼å®¹ï¼šé»˜èªåªè¿”å› DataFrame
        return df_aligned

    def predict_single(
        self,
        acc: float,
        time: float,
        pace: float,
        att: float,
        hw: float,
        e_read: float,
        e_vocab: float,
        e_logic: float,
        m_score: float,
    ) -> str:
        if self.xgb_model is None:
            if os.path.exists(config.MODEL_FILE_PREDICTION):
                self.xgb_model = joblib.load(config.MODEL_FILE_PREDICTION)
            else:
                return "âš ï¸ æ¨¡å‹å°šæœªè¨“ç·´"

        if self.label_encoder is None and os.path.exists(self.le_file_path):
            self.label_encoder = joblib.load(self.le_file_path)

        input_data = pd.DataFrame(
            {
                config.COL_ACCURACY: [acc],
                config.COL_AVG_TIME: [time],
                config.COL_LEARNING_PACE: [pace],
                config.COL_ATTENDANCE: [att],
                config.COL_HW_COMPLETION: [hw],
                "mean_score": [m_score],
                "err_reading": [e_read],
                "err_vocab": [e_vocab],
                "err_logic": [e_logic],
            }
        )

        try:
            pred_idx = self.xgb_model.predict(input_data[self._get_feature_columns()])[0]
            if self.label_encoder:
                return self.label_encoder.inverse_transform([pred_idx])[0]
            return str(pred_idx)
        except Exception as exc:
            return f"Error: {exc}"

    def run_analysis_pipeline(self, df: pd.DataFrame) -> tuple[pd.DataFrame, str]:
        if df.empty:
            return df, "ç„¡è³‡æ–™"

        df_clean = self.train_prediction_model(df)
        log: list[str] = [f"âœ… XGBoost è¨“ç·´å®Œæˆ (æœ‰æ•ˆè³‡æ–™: {len(df_clean)} ç­†)"]

        df_proc = self._preprocess_data(df_clean)
        predictions = self.xgb_model.predict(df_proc[self._get_feature_columns()])
        predicted_labels = self.label_encoder.inverse_transform(predictions)

        df_clean.loc[df_proc.index, config.COL_RECOMMENDED_LEVEL] = predicted_labels
        df_clean.loc[df_proc.index, config.COL_GROUP] = predicted_labels
        log.append("âœ… XGBoost æ¨è«–å®Œæˆ")

        return df_clean, "\n".join(log)

    def run_inference_only(self, df: pd.DataFrame) -> tuple[pd.DataFrame, str]:
        if df.empty:
            return df, "âŒ æª”æ¡ˆç‚ºç©º"

        df_proc = self._preprocess_data(df)
        if df_proc.empty:
            return df, "âŒ è³‡æ–™æ¸…æ´—å¾Œç‚ºç©º"

        df_clean = df.loc[df_proc.index].copy()

        if os.path.exists(config.MODEL_FILE_PREDICTION) and os.path.exists(self.le_file_path):
            self.xgb_model = joblib.load(config.MODEL_FILE_PREDICTION)
            self.label_encoder = joblib.load(self.le_file_path)

            predictions = self.xgb_model.predict(df_proc[self._get_feature_columns()])
            predicted_labels = self.label_encoder.inverse_transform(predictions)

            df_clean[config.COL_RECOMMENDED_LEVEL] = predicted_labels
            df_clean[config.COL_GROUP] = predicted_labels
            log = "âœ… èˆŠ XGBoost æ¨¡å‹æ¨è«–å®Œæˆ"
        else:
            df_clean[config.COL_RECOMMENDED_LEVEL] = "Unknown"
            df_clean[config.COL_GROUP] = "Unknown"
            log = "âš ï¸ ç„¡æ³•è¼‰å…¥æ¨¡å‹"

        return df_clean, log